{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67cd7148",
   "metadata": {},
   "source": [
    "# Movie Reviews Polarity Scoring using a RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53675c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# movie review \n",
    "# a recurrent neural network for movie reviews classification / polarity measure\n",
    "# \n",
    "# amir 2021 ...\n",
    "#\n",
    "from tensorflow.keras.preprocessing import text_dataset_from_directory\n",
    "from tensorflow.strings import regex_replace\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4096783f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepareData(dir):\n",
    "  data = text_dataset_from_directory(dir)\n",
    "  return data.map(\n",
    "    lambda text, label: (regex_replace(text, '<br />', ' '), label),\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ee636e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Found 25000 files belonging to 2 classes.\n",
      "b\"I took this out arbitrarily from the library the other night, having no idea of the film's cult, influence, or that it is currently being staged as a musical.(!) Most of the comments here are on target, it's moving, funny, sad, and yes, a tad exploitive despite the best intentions of the filmmakers. The expanded Chriterion edition is a must for anyone who loved it when it came out.   I think you can also see in little Edie the fall of a class that sort of disappeared, you can hear it in old films of Jackie O too; people just don't talk like that anymore. I think as a documentary, it would have been interesting to get more information about how the home fell into disrepute, Old Edie at least still seems aware of what's going on to a certain degree; couldn't She see the once spectacular home disintegrating?   Yet the film's subject is the life the two women have constructed for themselves now, a real life Tennesse Williams one act. Well worth your time.\"\n",
      "1\n",
      "Epoch 1/10\n",
      "782/782 [==============================] - 28s 34ms/step - loss: 0.5403 - accuracy: 0.7254\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.4502 - accuracy: 0.7948\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 28s 36ms/step - loss: 0.4108 - accuracy: 0.8130\n",
      "Epoch 4/10\n",
      "782/782 [==============================] - 28s 35ms/step - loss: 0.3860 - accuracy: 0.8272\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - 27s 35ms/step - loss: 0.3689 - accuracy: 0.8381\n",
      "Epoch 6/10\n",
      "782/782 [==============================] - 29s 37ms/step - loss: 0.3534 - accuracy: 0.8457\n",
      "Epoch 7/10\n",
      "782/782 [==============================] - 28s 36ms/step - loss: 0.3335 - accuracy: 0.8561\n",
      "Epoch 8/10\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 0.3293 - accuracy: 0.8579\n",
      "Epoch 9/10\n",
      "782/782 [==============================] - 33s 43ms/step - loss: 0.3072 - accuracy: 0.8683\n",
      "Epoch 10/10\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 0.2919 - accuracy: 0.8766\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x203e15c69a0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assumes you're in the root level of the dataset directory.\n",
    "# If you aren't, you'll need to change the relative paths here.\n",
    "train_data = prepareData('movie-reviews-dataset/train')\n",
    "test_data = prepareData('movie-reviews-dataset/test')\n",
    "\n",
    "for text_batch, label_batch in train_data.take(1):\n",
    "  print(text_batch.numpy()[0])\n",
    "  print(label_batch.numpy()[0]) # 0 = negative, 1 = positive\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# ----- 1. INPUT\n",
    "# We need this to use the TextVectorization layer next.\n",
    "model.add(Input(shape=(1,), dtype=\"string\"))\n",
    "\n",
    "# ----- 2. TEXT VECTORIZATION\n",
    "# This layer processes the input string and turns it into a sequence of\n",
    "# max_len integers, each of which maps to a certain token.\n",
    "max_tokens = 1000\n",
    "max_len = 100\n",
    "vectorize_layer = TextVectorization(\n",
    "  # Max vocab size. Any words outside of the max_tokens most common ones\n",
    "  # will be treated the same way: as \"out of vocabulary\" (OOV) tokens.\n",
    "  max_tokens=max_tokens,\n",
    "  # Output integer indices, one per string token\n",
    "  output_mode=\"int\",\n",
    "  # Always pad or truncate to exactly this many tokens\n",
    "  output_sequence_length=max_len,\n",
    ")\n",
    "\n",
    "# Call adapt(), which fits the TextVectorization layer to our text dataset.\n",
    "# This is when the max_tokens most common words (i.e. the vocabulary) are selected.\n",
    "train_texts = train_data.map(lambda text, label: text)\n",
    "vectorize_layer.adapt(train_texts)\n",
    "\n",
    "model.add(vectorize_layer)\n",
    "\n",
    "# ----- 3. EMBEDDING\n",
    "# This layer turns each integer (representing a token) from the previous layer\n",
    "# an embedding. Note that we're using max_tokens + 1 here, since there's an\n",
    "# out-of-vocabulary (OOV) token that gets added to the vocab.\n",
    "model.add(Embedding(max_tokens + 1, 128))\n",
    "\n",
    "# ----- 4. RECURRENT LAYER\n",
    "model.add(LSTM(64))\n",
    "\n",
    "# ----- 5. DENSE HIDDEN LAYER\n",
    "model.add(Dense(64, activation=\"relu\"))\n",
    "\n",
    "# ----- 6. OUTPUT\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "\n",
    "# Compile and train the model.\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.fit(train_data, epochs=10, verbose=1)     # -- was 10\n",
    "\n",
    "model.save_weights('rnn')\n",
    "\n",
    "model.load_weights('rnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a580381c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 76s 96ms/step - loss: 0.5491 - accuracy: 0.7808\n",
      "[[0.9795991]]\n",
      "[[0.01131651]]\n"
     ]
    }
   ],
   "source": [
    "# Try the model on our test dataset.\n",
    "model.evaluate(test_data)\n",
    "\n",
    "# Should print a very high score like 0.98.\n",
    "print(model.predict([\n",
    "  \"i loved it! highly recommend it to anyone and everyone looking for a great movie to watch.\",\n",
    "]))\n",
    "\n",
    "# Should print a very low score like 0.01.\n",
    "print(model.predict([\n",
    "  \"this was awful! i hated it so much, nobody should watch this. the acting was terrible, the music was terrible, overall it was just bad.\",\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cce09386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01795468]]\n"
     ]
    }
   ],
   "source": [
    "print(model.predict([\n",
    "  \"bad\",\n",
    "]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "102ded32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.9459926]]\n"
     ]
    }
   ],
   "source": [
    "print(model.predict([\n",
    "  \"I liked that.\",\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa588fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.94609725]]\n"
     ]
    }
   ],
   "source": [
    "print(model.predict([\n",
    "  \"such a nice movie.\",\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8fe67372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.8240361]]\n"
     ]
    }
   ],
   "source": [
    "print(model.predict([\n",
    "  \"I'm in love with that film.\",\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09c61d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.94364315]]\n"
     ]
    }
   ],
   "source": [
    "print(model.predict([\n",
    "  \"I love this movie very much but the song was average\",\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4d7a7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
